---
title: "AI Chips and Systems"
date: 2025-06-22
permalink: /posts/2025/06/blog-post-1/
tags:
  - resources
  - learning
  - AI
  - LLM 
---

## Fundamentals -- Model Architectures

Tokenizer  
BPE – Algorithm for tokenization (compression), SentencePiece Tokenizer: subword units (e.g. BPE) and unigram language model  
TikToken Tokenizer  
Very fast BPE Tokenizer, Used in Llama3 and OpenAI models, Tekken (derived from tiktoken) used by Mistral  

Absolute/Relative/Rotatory Positional Encoding  

Multi-Head Attention: [torch.nn.MultiheadAttention docs](https://docs.pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html)  
Linear Attention  
Grouped Multi-Query Attention / Multi-Query Attention: [mpt-7b-chat attention code](https://huggingface.co/mosaicml/mpt-7b-chat/blob/main/attention.py#L204)  
Faster Transformer Decoding: [One Write-Head is All you Need](https://arxiv.org/abs/1911.02150)  
Multi-query loss in quality but computationally fast  
GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints: [paper](https://arxiv.org/pdf/2305.13245)  

LLama2 RMS Norm, RoPE, Byte Pair with sentencepiece Tokenizer (llama3 Tiktoken Tokenizer), SwiGLU activation function  
Mixtral 7B Sliding window attention: [paper](https://arxiv.org/pdf/2310.06825)  

## Fundamentals -- GPU Coding

*(content to be filled in)*  

## MadMax -- can compiler be a smart LLM?

Optimizer (min latency, max throughput)  
Constraints by data, model and micro-architecture  
Function calling tools  
Input: model (pytorch)  
In-context: hardware architecture + SW stack + working paths  
Output: binary code, profiler graph  

TVM  
Meta's Chris Cummins Large Language Model for Compiler Optimization  
Adam Paszke Mosaic GPU a DSL for faster hopper + kernels in python: [video](https://www.youtube.com/watch?v=wKd90avC8Nc)  

## Compilers - XLA and HLOs

HLO IR via XLA  
Pallas: MLIR via Mosaic  
PJRT Plugins  

## TPU Hardware - Extreme Matmul throughput & Energy Efficiency 

GSPMD sharding  
Design: systolic array + pipeline, Ahead of Time compilation + less reliance on Cache  
[TPU deep dive](https://henryhmko.github.io/posts/tpu/tpu.html)  
[JAX scaling book](https://jax-ml.github.io/scaling-book/training/)  
Tensorcore (matmul and convo) Sparsecore (specializes in embedding lookups) and HBM  
[TPU Embedding API](https://www.tensorflow.org/recommenders/api_docs/python/tfrs/layers/embedding/TPUEmbedding)  
Tensor Parallelism with SPMD: jax.jit() ⇒ [paper](https://arxiv.org/pdf/1811.02084)  

## Tools - Modeling, Simulator and Profilers

Tools: Xprof / perfetto / buganizer / graphview  
[Xprof](https://github.com/openxla/xprof)  
[Perfetto](https://github.com/google/perfetto)  
[Buganizer](https://issuetracker.google.com/home)  
[Graphviz layouts](https://graphviz.org/docs/layouts/)  

Levels of simulation: TF Simulation, IR (Roofline estimator and heuristic estimator 20% variation)  
berkin akin, ISA level  
Tick-based estimator, SIM simulator  
Post-simulation: Roofline analyzer  

## Techniques in Inference

Goals of LLM Inference  

Speed: achieve low latency (TTFT, TPOT)  
Efficiency: achieve higher throughput (tokens/sec) and lower cost ($ / token)  
Ease of use: support more models, huggingface integration, API server …  

Warp Specialization: Inter-Warpgroup overlapping of GEMM and Softmax  
FlashAttention-3: [video](https://www.youtube.com/watch?v=_B6ZbRbxiMY)  
WGMMA and TMA: [Cutlass tutorial](https://research.colfax-intl.com/cutlass-tutorial-wgmma-hopper/) / [blog](https://tridao.me/blog/2024/flash3/)  
TMA memory loading (save registers / asynchronous)  
Sparse GPU Kernels for Deep Learning: [paper](https://arxiv.org/abs/2006.10901)  

Mixtral 8x7B: expert feed-forward layers: CUDA Megablock kernels for faster inference  

## SOTA Techniques in Inference

Token-wise continuous batching  
Highly-optimized TPU kernels for attention computation  
Flash-Attention for prefilling  
Paged-Attention for decoding  
Megablocks for MoE  
Quantization int8 and int4  
Streaming LLM and H2O cache eviction to support extreme long-context text generation  
Continuous batching  
Attention kernels (FlashAttention, PagedAttention, RaggedAttention, Megablox)  
Parallelism (model, data, pipeline)  
KV-cache management (block-wise + PagedAttention)  
KV-cache compression, eviction (StreamingLLM, H2O)  
Quantization (BnB, GPTQ, AWQ)  
Model Compilation (XLA, CUDA graph, torch.dynamo)  
Asynchronous machine (gRPC, Ray, Pathways)  
Latency balancing (chunked prefill, disaggregated serving)  
Model compression (Speculative decoding, distillation)  

vLLM: [video](https://www.youtube.com/watch?v=IqhJ5Eq8bgs)  
Optimized CUDA/Triton Kernels  
CUDA Graph to minimize host overheads  
Torch.Compile ⇒ Faster  

SGLang – RadixAttention?  

Speculative Decoding: Drafter + Primary (7B + 70B): [paper](https://arxiv.org/pdf/2211.17192)  
Prefill / Decoding HAO AI Lab: [video](https://www.youtube.com/watch?v=Bh-jlh5vlF0)  
Long-context retrieval KV-cache (1 KB to 80 KB per token): [blog](https://www.llamaindex.ai/blog/towards-long-context-rag)  
Prefill-Decode Disaggregation, vLLM  

PagedAttention Continuous Batching  
Pengchong Jin  
Flash attention for prefill  
PagedAttention for decode, Page Index ⇒ Token Blocks to remove internal fragmentation: [paper](https://arxiv.org/pdf/2309.06180)  

Megablox for MoE: [paper](https://arxiv.org/abs/2211.15841)  

TensorRT-LLM – Nvidia’s implementation of continuous batching, paged attention, multi-host support  
Databricks’ LLM inference performance: [blog](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices)  

## Techniques in Training / Tuning

Goal  
improve high throughput within AI cluster  

Approaches  
Highly efficient kernels (compute-bound)  
Model: Reduce communication & Improve MFU/HFU  
(PTD) Pipeline parallelism / Tensor parallelism / Data parallelism  
Improve network bandwidth  

Post-training GRPO | TRL – DeepSeek  
Unsloth -- Under the hood is Huggingface PEFT for training and SFT  
Dynamic LoRA  
PaLM: Scaling Language Modeling with Pathways: [paper](https://arxiv.org/abs/2204.02311)  
Megatron-LM -- Pretrain_gpt.py core/transformer/moe: [video](https://www.youtube.com/watch?v=TmUq0sFzgJY)  

VERL  
Model Parallelism Lianmin Zheng  
Ray & Amjad Almahairi  

Traditional SPMD assumes  
- Homogeneous hardware  
- Homogeneous model architecture;  
- Same parallelization and static resource allocation  

In reality, however,  
- Multimodal models → modality-specific parallelism  
- Cost Optimization → Hybrid hardware use  
- Complex pipelines → Flexible resource allocation  

Pathways learnings: Move from SPMD to MPMD  

An MPMD framework for distributed ML  
- Data processing, training and inference, but GPU support has been limited  
- Two new approaches for GPU orchestration  
  - Ray compiled Graphs  
  - Ray GPU Objects  
- Key benefits:  
  - Heterogeneous programs and accelerators  
  - Efficiency gains in various use cases  
  - Optimized GPU communication  
  - Open-source and production ready  
- Tasks: Future as the returned value executed asynchronously `@ray.remote`  
- Actors: a stateful method of task  
- Ray distributed object store  
- A lot of overheads ⇒ Ray  

## MoE

GShard: [paper](https://arxiv.org/pdf/2006.16668)  

## Quantization

AutoQuantizer:  
- Post-training Dynamic Quantization (PTDQ)  
- Post-training static Quantization (PTSQ)  
- Tuner – making a set of quantization configurations for search  
- Evaluator – takes configurations and starts quantization with quantization framework which implements quant/dequant methods and algorithms  
- JaxConverter – quantized Jax model into tflite model  
- Compiler and the profiler consumes tflite file to measure the latency on XPU and send feedbacks to evaluator and tuner  
- QAT trainer – training model with QAT algorithms  

Absmax quantization, zero point quantization (asymmetric quantization)  
Large Magnitude Features: [paper](https://arxiv.org/pdf/2208.07339)  
Hardamard Transform: Techniques to smooth out outlier features, reduce numeric errors  
Target operations:  
- Convolution, lax.conv, lax.conv_general_padding, lax.conv_general_dilated  
- Matmul, jax.numpy.matmul, jax.numpy.linalg.matmul  
- Einsum, jax.numpy.einsum  
- jax.lax.dot_general, jax.lax.linalg.tensordot  

## SOTA -- Model Architecture and Algorithms

Deepseek R1 -- Nvidia's implementation: [video](https://www.youtube.com/watch?v=ndGOaG2CC1A)  
Splash Attention: [kernel code](https://github.com/jax-ml/jax/blob/main/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_kernel.py)  

## DeepSeek v3

DeepSeek v3: training & inference codesign  
- Multi-Head Latent Attention (MLA) – reduce KV cache for inference  
- Mult-Token Prediction  
- Shared and routing experts  
- Large Scale fp8 training  
...  

## Benchmarks

[Artificial Analysis models](https://artificialanalysis.ai/models/)  
[LM Arena](https://lmarena.ai/)  

ShareGPT dataset: each request has different prompt length and max output length: [dataset](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/tree/main)  
Quality check: EleutherAI language model evaluation harness  
Approach: compare the evaluation scores against reference scores  

## Business

In-Datacenter Performance Analysis of a Tensor Processing Unit: [paper](https://arxiv.org/abs/1704.04760)  
OASIS: Generative Video Streamed Gaming — Still Costly  

## Systems

Slurm / GKE orchestrators  
Inference gateway and autoscaling  
Disaggregated serving, optimizations  

## Models 

google/gemma-3n-E4B-it-litert-preview  
Text diffusion: Large Language Diffusion Models: [paper](https://arxiv.org/abs/2502.09992)  
AudioLM  
