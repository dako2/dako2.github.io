---
title: "AI Notes -- Models, Chips and Systems"
date: 2025-06-22
permalink: /posts/2025/06/blog-post-1/
tags:
  - resources
  - learning
  - AI
  - LLM 
---

## Outline
* [MadMax -- can compiler be a smart LLM?](#madmax----can-compiler-be-a-smart-llm)
* [TPU Hardware - Extreme Matmul throughput & Energy Efficiency](#tpu-hardware---extreme-matmul-throughput--energy-efficiency)
* [Compilers - XLA and HLOs](#compilers---xla-and-hlos)
* [Tools - Modeling, Simulator and Profilers](#tools---modeling-simulator-and-profilers)
* [Techniques in Inference](#techniques-in-inference)
* [SOTA Techniques in Inference](#sota-techniques-in-inference)
* [Techniques in Training / Tuning](#techniques-in-training--tuning)
* [MoE](#moe)
* [Quantization](#quantization)
* [Fundamentals -- Model Architectures](#fundamentals----model-architectures)
* [Fundamentals -- GPU Coding](#fundamentals----gpu-coding)
* [SOTA -- Model Architecture and Algorithms](#sota----model-architecture-and-algorithms)
* [DeepSeek v3](#deepseek-v3)
* [Benchmarks](#benchmarks)
* [Business](#business)
* [Systems](#systems)
* [Models](#models)

---

## MadMax -- can compiler be a smart LLM?
* **Optimizer:** min latency, max throughput
* **Constraints:** data, model and micro-architecture
* **Function calling tools**
* **Input:** model (pytorch)
* **In-context:** hardware architecture + SW stack + working paths
* **Output:** binary code, profiler graph
* **References:**
    * TVM
    * Meta's Chris Cummins: [Large Language Model for Compiler Optimization](https://research.facebook.com/publications/large-language-model-for-compiler-optimization/)

## TPU Hardware - Extreme Matmul throughput & Energy Efficiency
* **GSPMD sharding**
* **Design:** systolic array (1970s) + pipeline, Ahead of Time compilation + less reliance on Cache
* **Links:**
    * [TPU Hardware Design](https://henryhmko.github.io/posts/tpu/tpu.html)
    * [Scaling ML with JAX](https://jax-ml.github.io/scaling-book/training/)
* **Components:** Tensorcore (matmul and convo) Sparsecore (specializes in embedding lookups) and HBM
* **TPUEmbedding:** [TensorFlow Recommenders API Docs](https://www.tensorflow.org/recommenders/api_docs/python/tfrs/layers/embedding/TPUEmbedding)
* **Tensor Parallelism with SPMD:** `jax.jit()` ⇒ [Paper](https://arxiv.org/pdf/1811.02084)
* **Norman p Jouppi**: MIPS [Talk](https://www.ac.uma.es/arith2024/slides/keynote2.pdf)
TPU v1 2013 TPU, 2014 training bottleneck thus training chip (memory, computation accuracy and special operators e.g. square roots, transcendental operators)
VLIW because compiler does everything scheduling instead of worrying about instruction scheduling in hardware.
ICI torus wire shared memory like Cray T3D Architecture, serdes
BF16 -- multiple two BF16 get FP24 and perform FP32 accumulation and get the identical results
OCS optical switch full duplex
John Hennessy's Turing Award Talk: The end of Dennard scaling is an equally important problem as the end of Moore's Law, but doesn't receive as much attention
48lanes 200Gig serdes ==> Higher dimensional torus
3 year lifecycle from spec to GA ... bf16, fp8, fp4, int (vegas bet)

## Compilers - XLA and HLOs
* **HLO IR** via XLA
* **Pallas:** MLIR via Mosaic 
* **Adam Paszke:** [Lecture 42: Mosaic GPU](http://www.youtube.com/watch?v=wKd90avC8Nc) - a DSL for faster hopper + kernels in python
* **PJRT Plugins**

## Inter-connect
Die-to-Die chiplet 
Pcie to 
TCP/IP to RDMA

UCIe / CCITA for Chiplet standardization

## Tools - Modeling, Simulator and Profilers
* **Tools:**
    * [Xprof](https://github.com/openxla/xprof)
    * [Perfetto](https://github.com/google/perfetto)
    * [Buganizer](https://issuetracker.google.com/home)
    * [Graphviz](https://graphviz.org/docs/layouts/)
* **Levels of simulation:** TF Simulation, IR (Roofline estimator and heuristic estimator 20% variation)
* **ISA level:** berkin akin
* **Tick-based estimator,** SIM simulator
* **Post-simulation:** Roofline analyzer 

## Techniques in Inference
* **Goals of LLM Inference**
    * **Speed:** achieve low latency (TTFT, TPOT)
    * **Efficiency:** achieve higher throughput (tokens/sec) and lower cost ($ / token)
    * **Easy of use:** support more models, huggingface integration, API server …
* **Techniques:**
    * **Warp Specialization:** Inter-Warpgroup overlapping of GEMM and Softmax
    * **FlashAttention-3:** [CUDA Mode Keynote | Tri Dao | Together.ai](http://www.youtube.com/watch?v=_B6ZbRbxiMY)
    * **WGMMA and TMA:** [CUTLASS Tutorial WGMMA Hopper](https://research.colfax-intl.com/cutlass-tutorial-wgmma-hopper/), [FlashAttention-3 Blog](https://tridao.me/blog/2024/flash3/)
    * **TMA memory loading:** save registers / asynchronous 
    * **Sparse GPU Kernels for Deep Learning:** [Paper (Graph Neural Network)](https://arxiv.org/abs/2006.10901)
    * **Mixtral 8x7B:** expert feed-forward layers: CUDA Megablock kernels for faster inference

## SOTA Techniques in Inference
* **Token-wise continuous batching** * **Highly-optimized TPU kernels** for attention computation
* **Flash-Attention** for prefilling
* **Paged-Attention** for decoding
* **Megablocks** for MoE
* **Quantization** int8 and int4
* **Streaming LLM and H2O cache eviction** to support extreme long-context text generation 
* **Continuous batching**
* **Attention kernels** (FlashAttention, PagedAttention, RaggedAttention, Megablox, Low-Rank Attention)
* **Parallelism** (model, data, pipeline)
* **KV-cache management** (block-wise + PagedAttention)
* **KV-cache compression, eviction** (StreamingLLM, H2O)
* **Quantization** (BnB, GPTQ, AWQ)
* **Model Compilation** (XLA, CUDA graph, torch.dynamo)
* **Asynchronous machine** (gRPC, Ray, Pathways)
* **Latency balancing** (chunked prefill, disaggregated serving)
* **Model compression** (Speculative decoding, distillation)
* **vLLM:** [CUDA Mode Keynote | Lily Liu | vLLM](http://www.youtube.com/watch?v=IqhJ5Eq8bgs)
    * Optimized CUDA/Triton Kernels 
    * CUDA Graph to minimize host overheads
    * `Torch.Compile` ⇒ Faster
* **SGLang** – RadixAttention?
* **Speculative Decoding:** Drafter + Primary (7B + 70B) ⇒ [Paper](https://arxiv.org/pdf/2211.17192)
* **Prefill / Decoding:** HAO AI Lab ⇒ [DistServe: disaggregating prefill and decoding for goodput-optimized LLM inference](http://www.youtube.com/watch?v=Bh-jlh5vlF0)
* **Long context retrieval:** KV-cache 1 KB to 80KB per token ⇒ [LlamaIndex Blog](https://www.llamaindex.ai/blog/towards-long-context-rag)
* **Prefill-Decode Disaggregation, VLLM**
* **PagedAttention Continuous Batching:** Pengchong jin
    * Flash attention for prefill
    * PagedAttention for decode, Page Index ⇒ Token Blocks to remove internal fragmentation ⇒ [Paper](https://arxiv.org/pdf/2309.06180)
* **Megablox for MoE:** [Paper](https://arxiv.org/abs/2211.15841)
* **Frameworks**:
**TensorRT-LLM:** Nvidia’s implementation of continuous batching, paged attention, multi-host support
- PTQ Static 
* **Databricks’ LLM inference performance:** [Blog Post](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices)

## Techniques in Training / Tuning
* **Goal:** improve high throughput within AI cluster
* **Approaches:**
    * Highly efficient kernels (compute-bound)
    * Model: Reduce communication & Improve MFU/HFU 
    * (PTD) Pipeline parallelism / Tensor parallelism  / Data parallelism
    * Improve network bandwidth   
* **Post-training GRPO | TRL – DeepSeek** * **Unsloth:** Under the hood is Huggingface PEFT for training and SFT
* **Dynamic LoRA**
* **PaLM:** [Scaling Language Modeling with Pathways](https://arxiv.org/abs/2204.02311)
* **Megatron-LM:** Pretrain_gpt.py core/transformer/moe ⇒ [分布式训练框架Megatron-LM代码概览](http://www.youtube.com/watch?v=TmUq0sFzgJY)
* **VERL**
* **Model Parallism:** Lianmin Zheng
* **Ray:** Amjad Almahairi 
    * **Traditional SPMD assumes:**
        * Homogeneous hardware 
        * Homogeneous model architecture; 
        * same parallelization and static resource allocation
    * **In reality, however:**
        * Multimodal models → modality-specific parallelism 
        * Cost Optimization → Hybrid hardware use
        * Complex pipelines → Flexible resource allocation 
    * **Pathways learnings:** Move from SPMD to MPMD 
    * **An MPMD framework for distributed ML**
        * Data processing, training and inference, but GPU support has been limited
    * **Two new approaches for GPU orchestration**
        * Ray compiled Graphs
        * Ray GPU Objects
    * **Key benefits:**
        * Heterogeneous programs and accelerators
        * Efficiency gains in various use cases
        * Optimized GPU communication
        * Open-source and production ready
    * **Tasks:** Future as the returned value executed asynchronously `@ray.remote` 
    * **Actors:** a stateful method of task 
    * **Ray distributed object store**
    * A lot of overheads ⇒ Ray 

## MoE
* **GShard:** [Paper](https://arxiv.org/pdf/2006.16668)

## Quantization
Important to reduce the cost of serving LLMs
Different precisions required for different layers and different functions; support all the different precisions in the accelerators. 

Use RL to run the precision reductions: Hyperscale Hardware Optimized Neural Architecture Search [Paper](https://taochen.me/files/h2o_nas-asplos23.pdf)

* **AutoQuantizer:** * **Post-training Dynamic Quantization (PTDQ)**
    * **Post-training static Quantization (PTSQ)**
    * **Tuner:** making a set of quantization configurations for search
    * **Evaluator:** takes configurations and starts quantization with quantization framework which implements quant/dequant methods and algorithms
    * **JaxConverter:** quantized Jax model into tflite model 
    * **Compiler and the profiler:** consumes tflite file to measure the latency on XPU and send feedbacks to evaluator and tuner 
    * **QAT trainer:** training model with QAT algorithms   
* **Techniques:**
    * **Absmax quantization, zero point quantization** (asymmetric quantization)
    * **Large Magnitude Features:** [Paper](https://arxiv.org/pdf/2208.07339)
    * **Hardamard Transform:** Techniques to smooth out outlier features Reduce numerics errors
* **Target operations:** Convolution, `lax.conv`, `lax.conv_general_padding`, `lax.conv_general_dilated`, Matmul, `jax.numpy.matmul`, `jax.numpy.linalg.matmul`, Einsum, `jax.numpy.einsum`, `jax.lax.dot_general`, `jax.lax.linalg.tensordot`

## Fundamentals -- Model Architectures
* **Tokenizer**
    * **BPE:** Algorithm for tokenization (compression)
    * **SentencePiece Tokenizer:** subword units (e.g. BPE) and unigram language model
    * **TikToken Tokenizer:** Very fast BPE Tokenizer, Used in Llama3 and OpenAI models
    * **Tekken:** (derived from tiktoken) used by Mistral
* **Positional Encoding:** Absolute/Relative/Rotatory Positional Encoding
* **Multi-Head Attention:** [PyTorch Docs](https://docs.pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html)
* **Linear Attention**
* **Grouped Multi-Query Attention / Multi-Query Attention:** [Hugging Face Code](https://huggingface.co/mosaicml/mpt-7b-chat/blob/main/attention.py#L204)
    * [Faster Transformer Decoding: One Write-Head is All you Need](https://arxiv.org/abs/1911.02150)
    * Multi-query: loss in quality but computationally fast
    * [GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](https://arxiv.org/pdf/2305.13245)
* **LLama2:** RMS Norm, RoPE, Byte Pair with sentencepiece Tokenizer (llama3 Tiktoken Tokenizer), SwiGLU activation function 
* **Mixtral 7B:** Sliding window attention ⇒ [Paper](https://arxiv.org/pdf/2310.06825)

## Fundamentals -- GPU Coding
*(No specific notes provided)*

## SOTA -- Model Architecture and Algorithms
* **Deepseek R1:** Nvidia's implementation ⇒ [DeepSeek R1 performance optimization](http://www.youtube.com/watch?v=ndGOaG2CC1A)
* **Splash Attention:** [JAX GitHub](https://github.com/jax-ml/jax/blob/main/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_kernel.py)

## DeepSeek v3
* **DeepSeek v3:** training & inference codesign
    * **Multi-Head Latent Attention (MLA):** reduce KV cache for inference
    * **Multi-Token Prediction**
    * **Shared and routing experts**
    * **Large Scale fp8 training**
    * ...

## Benchmarks
* [artificialanalysis.ai/models/](https://artificialanalysis.ai/models/)
* [lmarena.ai/](https://lmarena.ai/)
* **ShareGPT dataset:** each request has different prompt length and max output length ⇒ [Hugging Face Dataset](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/tree/main)
* **Quality check:** EleutherAI language model evaluation harness
    * **Approach:** compare the evaluation scores against reference scores
* **mlperf**

## Business
* [In-Datacenter Performance Analysis of a Tensor Processing Unit](https://arxiv.org/abs/1704.04760)
* **OASIS:** Generative Video Streamed Gaming — Still Costly

## Systems
* **Slurm / GKE** orchestrators
* **Inference gateway and autoscaling**
* **Disaggregated serving, Optimizations**
* **RAS Reliability Availability Servicability** use many checkpoints 
Map REduce problem 

## Models 
* `google/gemma-3n-E4B-it-litert-preview`
* **Text diffusion:** [Large Language Diffusion Models](https://arxiv.org/abs/2502.09992)
* **AudioLM**
